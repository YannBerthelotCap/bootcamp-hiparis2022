{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Any\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Get Train, val, test, and pred datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_target_split(df:pd.DataFrame, col:str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Split features and target values\n",
    "\n",
    "    Input:\n",
    "    data (pd.DataFrame) : input DataFrame with features and target values\n",
    "    col (str) : name of the target value column\n",
    "\n",
    "    Output:\n",
    "    X (pd.DataFrame): DataFrame with features\n",
    "    y (pd.DataFrame): DataFrame with target values\n",
    "    '''\n",
    "    X = df.drop(columns=[col])\n",
    "    y = df[[col]]  \n",
    "\n",
    "    return X, y\n",
    "\n",
    "def train_test_split(df:pd.DataFrame, col:str, ratio: float = 0.75) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Split inputs into 2 sets of data: training (train) and test (test).\n",
    "    Each set of data is splitted into features (X) and target values (y).\n",
    "\n",
    "    Input:\n",
    "    data (pd.DataFrame) : input DataFrame with features and target values\n",
    "    col (str) : name of the target value column\n",
    "    ratio (float) : split ratio, between 0 and 1, to split train and validation data\n",
    "\n",
    "    Output:\n",
    "    X_train (pd.DataFrame): DataFrame for training with features\n",
    "    y_train (pd.DataFrame): DataFrame for training with target values\n",
    "    X_test (pd.DataFrame): DataFrame for testing with features\n",
    "    y_test (pd.DataFrame): DataFrame for testing with target values\n",
    "    '''\n",
    "    # spint train and test sets\n",
    "    df = df.sort_values(by=[\"DISCOVERY_DATE\"])\n",
    "    index_ratio = int(ratio * df.shape[0]) # find the row number where we want to split\n",
    "    split_date = df.iloc[index_ratio, df.columns.get_loc(\"DISCOVERY_DATE\")] # find the corresponding data\n",
    "    data_train = df[df[\"DISCOVERY_DATE\"] < split_date].drop(columns=[\"DISCOVERY_DATE\"]).copy()\n",
    "    data_test = df[df[\"DISCOVERY_DATE\"] >= split_date].drop(columns=[\"DISCOVERY_DATE\"]).copy()\n",
    "\n",
    "    # split between features and target values\n",
    "    X_train, y_train = features_target_split(data_train, target_col)\n",
    "    X_test, y_test = features_target_split(data_test, target_col)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "data = pd.read_csv(\"./data/4_input_model/model_1.csv\", index_col=[\"FOD_ID\"], parse_dates=[\"DISCOVERY_DATE\"])\n",
    "min_year_pred = 2015\n",
    "target_col = \"CAUSE_CODE\"\n",
    "ratio = 0.75\n",
    "\n",
    "# find max number of occurrence per day\n",
    "max_occ_day = data.groupby(\"DISCOVERY_DATE\").agg({\"FIRE_YEAR\":\"count\"}).max().values[0]\n",
    "\n",
    "# split train and prediction datasets\n",
    "X_pred = data[data[\"FIRE_YEAR\"] >= min_year_pred].copy()\n",
    "X_pred.drop(columns=[\"CAUSE_CODE\", \"DISCOVERY_DATE\"], inplace=True) # get features for predictions\n",
    "data = data[data[\"FIRE_YEAR\"] < min_year_pred].copy() # get train data (features and target values)\n",
    "data[target_col] = data[target_col].astype(\"int\")\n",
    "\n",
    "# split train, val and test sets\n",
    "X_train_val, y_train_val, X_test, y_test = train_test_split(data, target_col, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Use Train data for parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_predict(X_train:pd.DataFrame, y_train:pd.DataFrame, X_val:pd.DataFrame) -> Tuple[pd.DataFrame, Any]:\n",
    "    '''\n",
    "    Create a model, fit it on X_train and y_train, and predict the target values from X_val\n",
    "\n",
    "    Input:\n",
    "    X_train (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train (pd.DataFrame) : input DataFrame with target values for training\n",
    "    X_val (pd.DataFrame) : input DataFrame with features for validation\n",
    "\n",
    "    Output:\n",
    "    y_pre (pd.DataFrame): predictions based on the features from X_val\n",
    "    model : trained model\n",
    "    '''\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    y_pre = model.predict(X_val)\n",
    "    return y_pre, model\n",
    "\n",
    "\n",
    "def scoring(y_true:pd.DataFrame, y_pre:pd.DataFrame) -> dict:\n",
    "    '''\n",
    "    Return a dictionary with keys corresponding to score name and values corresponding to the associated score\n",
    "\n",
    "    Input:\n",
    "    y_true (pd.DataFrame) : input DataFrame with true labels\n",
    "    y_pre (pd.DataFrame) : input DataFrame with predicted labels\n",
    "\n",
    "    Output:\n",
    "    (dict): output dictionary with scores\n",
    "    '''\n",
    "    return {\n",
    "        \"f1-micro\": f1_score(y_true, y_pre, average=\"micro\"),\n",
    "        \"f1-macro\": f1_score(y_true, y_pre, average=\"macro\"),\n",
    "        \"f1-weighted\": f1_score(y_true, y_pre, average=\"weighted\"),\n",
    "        \"accuracy\": accuracy_score(y_true, y_pre)\n",
    "    }\n",
    "\n",
    "\n",
    "def print_scoring(scores:dict) -> None:\n",
    "    '''\n",
    "    Print scores from a dictionary\n",
    "\n",
    "    Input:\n",
    "    scores (dict) : dictionary with keys corresponding to score name and values corresponding to the associated score\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    for name, score in scores.items():\n",
    "        print(f\"{name}: {score}\")\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_test:pd.DataFrame, y_pre:pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Plot the confusion matrix based on the provided true and predicted labels\n",
    "\n",
    "    Input:\n",
    "    y_true (pd.DataFrame) : input DataFrame with true labels\n",
    "    y_pre (pd.DataFrame) : input DataFrame with predicted labels\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    sns.heatmap(\n",
    "        confusion_matrix(y_test, y_pre),\n",
    "        xticklabels=range(4),\n",
    "        yticklabels=range(4),\n",
    "        annot=True,\n",
    "        cmap='Blues',\n",
    "        fmt='g',\n",
    "        cbar=False\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def rf_features_importance(model: Any, cols:list) -> None:\n",
    "    '''\n",
    "    Plot feature_importance from a random forest model\n",
    "\n",
    "    Input:\n",
    "    y_true (pd.DataFrame) : input DataFrame with true labels\n",
    "    y_pre (pd.DataFrame) : input DataFrame with predicted labels\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    # get feature importance from model\n",
    "    importances = model.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=cols).sort_values(ascending=False)[:10]\n",
    "    # plot results\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances.plot.bar(ax=ax)\n",
    "    ax.set_title(\"Feature importances\")\n",
    "    fig.tight_layout()\n",
    " \n",
    "\n",
    "def cross_validation_score(X_train_val:pd.DataFrame, y_train_val:pd.DataFrame, gap:int) -> Any:\n",
    "    '''\n",
    "    Cross validation for time series, with 5 splits\n",
    "\n",
    "    Input:\n",
    "    X_train_val (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train_val (pd.DataFrame) : input DataFrame with target values for training\n",
    "    gap (int) : number of entries between 2 sets during the cross-validation\n",
    "\n",
    "    Output:\n",
    "    model: trained model\n",
    "    '''\n",
    "    # cross-validation for time series\n",
    "    tscv = TimeSeriesSplit(gap=gap, n_splits=3)\n",
    "    y_pre, y_val, model = None, None, None\n",
    "    scores_history = []\n",
    "    i = 0\n",
    "    for train_index, val_index in tscv.split(X_train_val):\n",
    "        i += 1 # increase iteration\n",
    "        # get datasets (train and val)\n",
    "        train_index = list(train_index)\n",
    "        val_index = list(val_index)\n",
    "        X_train, X_val = X_train_val.iloc[train_index, :], X_train_val.iloc[val_index, :]\n",
    "        y_train, y_val = y_train_val.iloc[train_index, :], y_train_val.iloc[val_index, :]\n",
    "        # fit model and predict y_val\n",
    "        y_pre, model = model_fit_predict(X_train, y_train, X_val)\n",
    "        # scoring\n",
    "        scores = scoring(y_val, y_pre)\n",
    "        scores_history.append(scores)\n",
    "        print(f\"Step {i}\")\n",
    "        print_scoring(scores)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-micro: 0.5606276490215529\n",
      "f1-macro: 0.5051460891433592\n",
      "f1-weighted: 0.5477537688633421\n",
      "accuracy: 0.5606276490215529\n",
      "f1-micro: 0.5830462620615023\n",
      "f1-macro: 0.5348924767751648\n",
      "f1-weighted: 0.5732697638902331\n",
      "accuracy: 0.5830462620615023\n",
      "f1-micro: 0.618198214446749\n",
      "f1-macro: 0.5647393489416004\n",
      "f1-weighted: 0.6073772102796127\n",
      "accuracy: 0.618198214446749\n"
     ]
    }
   ],
   "source": [
    "# train model on X_train_val and y_train_val\n",
    "model = cross_validation_score(X_train_val, y_train_val, gap = max_occ_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Test the model on the Test set, and refit the model on the entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be done only few times (risk of overfitting)\n",
    "\n",
    "def score_test_set(model:Any, X_train_val:pd.DataFrame, y_train_val:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Fit the model on the entire X_train_val and y_train_val data, and predict values for the test set\n",
    "\n",
    "    Input:\n",
    "    model : model used for training the the previous section of the notebook\n",
    "    X_train_val (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train_val (pd.DataFrame) : input DataFrame with target values for training\n",
    "    X_test (pd.DataFrame) : input DataFrame with features for testing\n",
    "    y_test (pd.DataFrame) : input DataFrame with target values for testing\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    # fit model on entire X_train_val, y_train_val datasets\n",
    "    model.fit(X_train_val, y_train_val.values.ravel())\n",
    "    y_pred = model.predict(X_test)\n",
    "    # score test set\n",
    "    scores = scoring(y_test, y_pred)\n",
    "    print_scoring(scores)\n",
    "    # plot the last confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred)\n",
    "    # plot features importance\n",
    "    cols = X_train_val.columns # get columns names\n",
    "    rf_features_importance(model, cols) # plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test_set(model, X_train_val, y_train_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Create the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fbettini\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- DISCOVERY_DATE\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The DTypes <class 'numpy.dtype[datetime64]'> and <class 'numpy.dtype[int64]'> do not have a common DType. For example they cannot be stored in a single array unless the dtype is `object`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22444\\4007087748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"submission_group_X\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0msave_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22444\\4007087748.py\u001b[0m in \u001b[0;36msave_predictions\u001b[1;34m(X_pred, model, filename)\u001b[0m\n\u001b[0;32m     12\u001b[0m     '''\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# predict target values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# create a DataFrame with results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     submission = pd.DataFrame(\n",
      "\u001b[1;32mc:\\Users\\fbettini\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    806\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m--> 808\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fbettini\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 850\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fbettini\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    577\u001b[0m         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n\u001b[0;32m    578\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No support for np.int64 index based sparse matrices\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fbettini\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fbettini\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 665\u001b[1;33m             \u001b[0mdtype_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The DTypes <class 'numpy.dtype[datetime64]'> and <class 'numpy.dtype[int64]'> do not have a common DType. For example they cannot be stored in a single array unless the dtype is `object`."
     ]
    }
   ],
   "source": [
    "def fit_model_all_data(model:Any, X_train_val:pd.DataFrame, y_train_val:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame):\n",
    "    '''\n",
    "    Fit the model on the entire X_train_val and X_test features\n",
    "\n",
    "    Input:\n",
    "    model : model used for training the the previous section of the notebook\n",
    "    X_train_val (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train_val (pd.DataFrame) : input DataFrame with target values for training\n",
    "    X_test (pd.DataFrame) : input DataFrame with features for testing\n",
    "    y_test (pd.DataFrame) : input DataFrame with target values for testing\n",
    "\n",
    "    Output:\n",
    "    model : model trained on all available data\n",
    "    '''\n",
    "    # fit model on entire X_train_val, y_train_val datasets\n",
    "    X = pd.concat([X_train_val, X_test])\n",
    "    y = pd.concat([y_train_val, y_test])\n",
    "    model.fit(X, y.values.ravel())\n",
    "    return model\n",
    "    \n",
    "\n",
    "def save_predictions(X_pred:pd.DataFrame, model, filename:str) -> None:\n",
    "    '''\n",
    "    Save predictions (year 2015) to csv format, based on a provided pre-trained model, and features for predictions X_pred\n",
    "\n",
    "    Input:\n",
    "    X_pred (pd.DataFrame) : input DataFrame with features for predictions\n",
    "    model : input pre-trained model, which has a predict method\n",
    "    filename (str) : Name of the file for submission\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    # predict target values\n",
    "    y_pred = model.predict(X_pred)\n",
    "    # create a DataFrame with results\n",
    "    submission = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        index=X_pred.index,\n",
    "        columns=[\"CAUSE_CODE\"]\n",
    "    ).reset_index()\n",
    "    # save to csv\n",
    "    submission.to_csv(f\"./data/5_predictions/{filename}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([201844333, 201863160, 201760655, 201844334, 201841540, 201851228,\n",
       "            201865229, 201855311, 201870432, 201843165,\n",
       "            ...\n",
       "            300088190, 300130992, 300083301, 300085069, 300106931, 300085469,\n",
       "            300146049, 300132989, 300138823, 300129763],\n",
       "           dtype='int64', name='FOD_ID', length=74073)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"submission_group_X\"\n",
    "model = fit_model_all_data(model, X_train_val, y_train_val, X_test, y_test)\n",
    "save_predictions(X_pred, model, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f17db48aa853750bfee38181acc93506773951f4f6f179b65dfa4e5104417bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

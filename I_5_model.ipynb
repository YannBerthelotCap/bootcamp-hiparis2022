{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"./figures/logo-hi-paris-retina.png\" alt=\"Logo\" width=\"280\" height=\"180\">\n",
    "  </a>\n",
    "\n",
    "  <h3 align=\"center\">Data Science Bootcamp</h3>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors : Yann Berthelot, Florian Bettini, Laure-Am√©lie Colin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Val / Test Split\n",
    "The **train-validation-test** split is a technique for training and evaluating the performance of a machine learning algorithm.\n",
    "\n",
    "The **procedure** involves taking a dataset and dividing it into three subsets:\n",
    "- The **train** subset is used to fit the model and is referred to as the training dataset. You should not evaluate the performance of the model on this train set,\n",
    "- The **validation** subset is used to tune (select the best) hyperparameters of an algorithm. For example the *max_depth* for a regression tree. It is used to compare different versions of the model with different hyperparameters, but not to evaluate it.\n",
    "- The **test** subset is not used to train the model; it is only used at the end to evaluate the performances of the model. For a purist, once you 'opened' the test set, you should not modify the model anymore.  \n",
    "\n",
    "Test set cannot be used for validation as it could lead to overifitting while selecting amongst different models with different hyperparameters. It should only be used for evaluation and not for tuning.\n",
    "\n",
    "**Strategy**\n",
    "   - Define a test set, and separate the remaining between train and validation.\n",
    "   - Generally they represent respectively 70% | 15% | 15% of the initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/train_test_1.png\" width=700 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However there is a specificity to our problem as we are dealing with timeseries. By shuffling the data and randomly selecting examples to build our different sets we would be facing a problem of leaking future information with past information :  \n",
    "If we randomly selected data points for our training set and test set we would have fires in the train set that would be of later dates than some in the test set. This would mean that the model learnt from future informations and thus is \"cheating\" since it already knows some of what happens next (e.g. there was a serie of fire afterwards). This is called leaking and we want to avoid it.\n",
    "\n",
    "As a consequence, we will be using a different type of split made for time series : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/TimeSeriesSplit.png\" width=700 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines 0 to 3 are our different split. The data is organized in chronological order. We first use a portion of the data to train, and then test on the following portion of data (line 0). We then repeat this process by adding the previous testing set to our training set and using the following portion as our testing set. We then repeat this operation until we have covered the whole dataset.  \n",
    "Some small adjusment which can be made is to add a \"gap\" between the training and testing set. Since we are dealing with time series, following samples can be highly dependent (e.g. we have two consecutive data points in the same part of the country, the first fire having created the second). If we split between this two point we would have the illusion of testing our model on independent data while we would have been lucky on this example. In order to prevent this effect of dependence we add a gap of discarded data between the training set and testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/TimeSeriesGap.png\" width=700 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features / Target Split\n",
    "It is a rather practical approach because generally the algorithms of machine learning ask for the features on the one hand and the target on the other hand.\n",
    "\n",
    "<img src=\"./figures/split_columns.png\" width=700 height=500 />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Bias / Variance Trade-off](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
    "\n",
    "<img src=\"./figures/Overfitting.PNG\" width=600 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{UNDERFITTING :}$ The model is too simple to capture the relationships between the data\n",
    "\n",
    "*Solutions*:\n",
    "- Introduce more features\n",
    "- Increase model complexity\n",
    "\n",
    "$\\textbf{OVERFITTING :}$ The model is too complex and sticks too closely to the training data\n",
    "\n",
    "*Solutions*:\n",
    "\n",
    "- Decrease model complexity\n",
    "- Include more data\n",
    "- Use regularization\n",
    "\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "The main problem with decision trees is their large variance: a tiny error at the top of the tree is propagated all the way down the tree and it gets worse quickly. To stabilize the tree's predictions, we prefer to generate a set of trees, a forest and this algorithm is called *Random Forest*.\n",
    "\n",
    "To create a random forest with B trees, we proceed as follows:\n",
    "\n",
    "- For i ranging from 1 to B:\n",
    "  - We draw randomly with replacement a sub-sample of the data size $ n <n _ {\\ text {train}} $\n",
    "  - We randomly draw a subsample of features of size m with in general $ m \\leq \\sqrt {p} $\n",
    "  - On this new dataset composed of n examples and m features, we train a decision tree of fixed max depth\n",
    "- We thus obtain $ B $ decision trees. If we denote by $ f1, ..., fB $ the prediction functions of each tree, then in regression, the decision function of the forest $ f_ {RF} $ will be:\n",
    "\n",
    "$f_{RF} (x) = \\frac{1}{B} \\sum_{i = 1, ..., B} f_i (x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/RF.png\" width=900 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mains parameters\n",
    "\n",
    "- *n_estimators* : number of trees in the foreset\n",
    "\n",
    "- *max_features* : max number of features considered for splitting a node\n",
    "\n",
    "- *max_depth* : max number of levels in each decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective of this lab\n",
    "======\n",
    "\n",
    "Experiment with multiple models and hyperparameters and select the one with the best score.\n",
    "Use Cross-validation (among other methods) to avoid overfitting.\n",
    "Use explainability to understand and present the model's predictions.\n",
    "\n",
    "We will use the [sklearn package](https://scikit-learn.org/stable/) for models and various metrics. You also have the possibility to use the [XGBoost](https://xgboost.readthedocs.io/en/stable/) model if you feel confident with it, it has the nice property of allowing to use the same methods as sklearn models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies with previous Labs\n",
    "\n",
    "This lab uses one dataset from a previous Lab:\n",
    "- a dataset ready to use for an ML model `./data/3_input_model/input_model.csv`\n",
    "\n",
    "You can either:\n",
    "- [preferred option] start over from the work that you produced\n",
    "- or take pre-processed datasets located in `./data/9_helper_datasets`. In that case:\n",
    "    - `./data/9_helper_datasets/input_model.csv` should be **copied** (not deleted) to `./data/3_input_model/input_model.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "\n",
    "# Typing and error catching\n",
    "from functools import partial\n",
    "from typing import Tuple, Any, Union\n",
    "from shap.utils._exceptions import InvalidModelError\n",
    "MODEL_TYPE = Union[LogisticRegression, RandomForestClassifier, XGBClassifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Get Train, val, test, and pred datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "data = pd.read_csv(\"./data/3_input_model/input_model.csv\", parse_dates=[\"DISCOVERY_DATE\"])\n",
    "min_year_pred = \"2015-01-01\"\n",
    "target_col = \"FIRE\"\n",
    "ratio = 0.75\n",
    "\n",
    "# find max number of occurrence per day\n",
    "max_occ_day = data.groupby(\"DISCOVERY_DATE\").agg({\"STATE\":\"count\"}).max().values[0]\n",
    "\n",
    "# split train and prediction datasets\n",
    "X_pred = data[data[\"DISCOVERY_DATE\"] >= min_year_pred].set_index([\"DISCOVERY_DATE\", \"STATE\"]).copy()\n",
    "X_pred.drop(columns=[\"FIRE\"], inplace=True) # get features for predictions\n",
    "data = data[data[\"DISCOVERY_DATE\"] < min_year_pred].copy() # get train data (features and target values)\n",
    "data[target_col] = data[target_col].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_target_split(df:pd.DataFrame, col:str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Split features and target values. \n",
    "    It is meant to help you in the next function, you can skip if you feel like it\n",
    "\n",
    "    Input:\n",
    "    data (pd.DataFrame) : input DataFrame with features and target values\n",
    "    col (str) : name of the target value column\n",
    "\n",
    "    Output:\n",
    "    X (pd.DataFrame): DataFrame with features\n",
    "    y (pd.DataFrame): DataFrame with target values\n",
    "    '''\n",
    "    #################\n",
    "    # TO BE FILLED  #\n",
    "    #################\n",
    "\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test features_target_split\n",
    "target_col = \"FIRE\"\n",
    "X_train, y_train = features_target_split(data, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df:pd.DataFrame, col:str, ratio: float = 0.75) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Split inputs into 2 sets of data: training (train) and test (test).\n",
    "    Each set of data is splitted into features (X) and target values (y).\n",
    "    Be careful not to split with shuffling as we need to keep the chronological order\n",
    "    \n",
    "\n",
    "    Input:\n",
    "    data (pd.DataFrame) : input DataFrame with features and target values\n",
    "    col (str) : name of the target value column\n",
    "    ratio (float) : split ratio, between 0 and 1, to split train and validation data\n",
    "\n",
    "    Output:\n",
    "    X_train (pd.DataFrame): DataFrame for training with features\n",
    "    y_train (pd.DataFrame): DataFrame for training with target values\n",
    "    X_test (pd.DataFrame): DataFrame for testing with features\n",
    "    y_test (pd.DataFrame): DataFrame for testing with target values\n",
    "    '''\n",
    "    # spint train and test sets\n",
    "    df = df.sort_values(by=[\"DISCOVERY_DATE\"])\n",
    "    index_ratio = int(ratio * df.shape[0]) # find the row number where we want to split\n",
    "    split_date = df.iloc[index_ratio, df.columns.get_loc(\"DISCOVERY_DATE\")] # find the corresponding data\n",
    "    data_train = df[df[\"DISCOVERY_DATE\"] < split_date].set_index([\"DISCOVERY_DATE\", \"STATE\"]).copy()\n",
    "    data_test = df[df[\"DISCOVERY_DATE\"] >= split_date].set_index([\"DISCOVERY_DATE\", \"STATE\"]).copy()\n",
    "\n",
    "    # split between features and target values\n",
    "    X_train, y_train = features_target_split(data_train, col)\n",
    "    X_test, y_test = features_target_split(data_test, col)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, val and test sets\n",
    "X_train_val, y_train_val, X_test, y_test = train_test_split(data, target_col, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Use Train data for parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_predict(model:MODEL_TYPE, X_train:pd.DataFrame, y_train:pd.DataFrame, X_val:pd.DataFrame) -> Tuple[pd.DataFrame, Any]:\n",
    "    '''\n",
    "    Create a model, fit it on X_train and y_train, and predict the target values from X_val\n",
    "\n",
    "    Input:\n",
    "    model (MODEL_TYPE) : The model to fit and to then use for predictions on the validation set\n",
    "    X_train (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train (pd.DataFrame) : input DataFrame with target values for training\n",
    "    X_val (pd.DataFrame) : input DataFrame with features for validation\n",
    "\n",
    "    Output:\n",
    "    y_pre (pd.DataFrame): predictions based on the features from X_val\n",
    "    model : trained model\n",
    "    '''\n",
    "    if \"fit\" in dir(model):\n",
    "        #################\n",
    "        # TO BE FILLED  #\n",
    "        #################\n",
    "        # Fit model using X_train and y_train\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Model not supported by this helper function. You have to write your own code')\n",
    "    if \"predict\" in dir(model):\n",
    "        #################\n",
    "        # TO BE FILLED  #\n",
    "        #################\n",
    "        # Get the preditions on X_val using the model\n",
    "    else:\n",
    "        raise ValueError('Model not supported by this helper function. You have to write your own code')\n",
    "    return y_pre, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model_fit_predict\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "y_pre, trained_model = model_fit_predict(random_forest, X_train_val, y_train_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(y_true:pd.DataFrame, y_pre:pd.DataFrame) -> dict:\n",
    "    '''\n",
    "    Return a dictionary with keys corresponding to score name and values corresponding to the associated score\n",
    "\n",
    "    Input:\n",
    "    y_true (pd.DataFrame) : input DataFrame with true labels\n",
    "    y_pre (pd.DataFrame) : input DataFrame with predicted labels\n",
    "\n",
    "    Output:\n",
    "    (dict): output dictionary with scores\n",
    "    '''\n",
    "    return {\n",
    "        \"f1-micro\": f1_score(y_true, y_pre, average=\"micro\"),\n",
    "        \"f1-macro\": f1_score(y_true, y_pre, average=\"macro\"),\n",
    "        \"f1-weighted\": f1_score(y_true, y_pre, average=\"weighted\"),\n",
    "        \"accuracy\": accuracy_score(y_true, y_pre)\n",
    "    }\n",
    "\n",
    "def print_scoring(scores:dict) -> None:\n",
    "    '''\n",
    "    Print scores from a dictionary\n",
    "\n",
    "    Input:\n",
    "    scores (dict) : dictionary with keys corresponding to score name and values corresponding to the associated score\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    for name, score in scores.items():\n",
    "        print(f\"{name}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scoring\n",
    "scores = scoring(y_test, y_pre)\n",
    "print_scoring(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test:pd.DataFrame, y_pre:pd.DataFrame, nb_values:int) -> None:\n",
    "    '''\n",
    "    Plot the confusion matrix based on the provided true and predicted labels\n",
    "\n",
    "    Input:\n",
    "    y_true (pd.DataFrame) : input DataFrame with true labels\n",
    "    y_pre (pd.DataFrame) : input DataFrame with predicted labels\n",
    "    nb_values (int) : nb of target values\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    s = sns.heatmap(\n",
    "        confusion_matrix(y_test, y_pre),\n",
    "        xticklabels=range(nb_values),\n",
    "        yticklabels=range(nb_values),\n",
    "        annot=True,\n",
    "        cmap='Blues',\n",
    "        fmt='g',\n",
    "        cbar=False\n",
    "    )\n",
    "    s.set(xlabel='True label', ylabel='Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pre, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_features_importance(model: MODEL_TYPE, cols:list) -> None:\n",
    "    '''\n",
    "    Plot feature_importance from a random forest model\n",
    "\n",
    "    Input:\n",
    "    model (MODEL_TYPE) : Model for which to compute the feature importance\n",
    "    cols (list) : input DataFrame with true labels\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    # get feature importance from model\n",
    "    importances = model.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=cols).sort_values(ascending=False)[:10]\n",
    "    # plot results\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_importances.plot.bar(ax=ax)\n",
    "    ax.set_title(\"Feature importances\")\n",
    "    fig.tight_layout()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test random forest feature importance\n",
    "rf_features_importance(random_forest, cols=X_train_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_score(model: MODEL_TYPE, X_train_val:pd.DataFrame, y_train_val:pd.DataFrame, gap:int) -> Tuple[MODEL_TYPE, dict]:\n",
    "    '''\n",
    "    Cross validation for time series, with 5 splits.\n",
    "\n",
    "    Input:\n",
    "    X_train_val (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train_val (pd.DataFrame) : input DataFrame with target values for training\n",
    "    gap (int) : number of entries between 2 sets during the cross-validation\n",
    "\n",
    "    Output:\n",
    "    model: trained model\n",
    "    '''\n",
    "    # cross-validation for time series\n",
    "    tscv = TimeSeriesSplit(n_splits=3, gap=gap)\n",
    "    y_pre, y_val, trained_model = None, None, None\n",
    "    scores_history = []\n",
    "    i = 0\n",
    "    for train_index, val_index in tscv.split(X_train_val):\n",
    "        i += 1 # increase iteration\n",
    "        # get datasets (train and val)\n",
    "        train_index = list(train_index)\n",
    "        val_index = list(val_index)\n",
    "        X_train, X_val = X_train_val.iloc[train_index, :], X_train_val.iloc[val_index, :]\n",
    "        y_train, y_val = y_train_val.iloc[train_index, :], y_train_val.iloc[val_index, :]\n",
    "        # fit model and predict y_val\n",
    "        y_pre, trained_model = model_fit_predict(model, X_train, y_train, X_val)\n",
    "        # scoring\n",
    "        scores = scoring(y_val, y_pre)\n",
    "        scores_history.append(scores)\n",
    "        print(\"\")\n",
    "        print(f\"Step {i}\")\n",
    "        print_scoring(scores)\n",
    "\n",
    "    return trained_model, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression, logistic_scores = cross_validation_score(logistic_regression, X_train_val, y_train_val, gap = max_occ_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model on X_train_val and y_train_val\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest, random_forest_scores = cross_validation_score(random_forest, X_train_val, y_train_val, gap = max_occ_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "xgb_model, xgb_scores = cross_validation_score(xgb_model, X_train_val, y_train_val, gap = max_occ_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_params = {\"n_estimators\":[100,200], 'min_samples_split': [2, 5, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest = GridSearchCV(random_forest, random_forest_params, scoring=None, n_jobs=None, refit=True, cv=TimeSeriesSplit(n_splits=3, gap=max_occ_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "        'min_child_weight': [5, 10],\n",
    "        'gamma': [1, 2],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = GridSearchCV(xgb_model, xgb_params, scoring=None, n_jobs=None, refit=True, cv=TimeSeriesSplit(n_splits=3, gap=max_occ_day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Test the model on the Test set, and refit the model on the entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be done only few times (risk of overfitting)\n",
    "def score_test_set(model:Any, X_train_val:pd.DataFrame, y_train_val:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Fit the model on the entire X_train_val and y_train_val data, and predict values for the test set\n",
    "\n",
    "    Input:\n",
    "    model (MODEL_TYPE) : model used for training the the previous section of the notebook\n",
    "    X_train_val (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train_val (pd.DataFrame) : input DataFrame with target values for training\n",
    "    X_test (pd.DataFrame) : input DataFrame with features for testing\n",
    "    y_test (pd.DataFrame) : input DataFrame with target values for testing\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    # fit model on entire X_train_val, y_train_val datasets\n",
    "    model.fit(X_train_val, y_train_val.values.ravel())\n",
    "    y_pred = model.predict(X_test)\n",
    "    # score test set\n",
    "    scores = scoring(y_test, y_pred)\n",
    "    print_scoring(scores)\n",
    "    # plot the last confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred, nb_values=2)\n",
    "    # plot features importance\n",
    "    cols = X_train_val.columns # get columns names\n",
    "    if 'feature_importances_' in dir(model):\n",
    "        rf_features_importance(model, cols) # plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test_set(random_forest, X_train_val, y_train_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model in (logistic_regression, xgb_model, random_forest, best_random_forest, best_xgb):\n",
    "    print(model.__repr__().split('(')[0])\n",
    "    score_test_set(model, X_train_val, y_train_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Create the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_all_data(model: MODEL_TYPE, X_train_val:pd.DataFrame, y_train_val:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame):\n",
    "    '''\n",
    "    Fit the model on the entire X_train_val and X_test features\n",
    "\n",
    "    Input:\n",
    "    model : model used for training the the previous section of the notebook\n",
    "    X_train_val (pd.DataFrame) : input DataFrame with features for training\n",
    "    y_train_val (pd.DataFrame) : input DataFrame with target values for training\n",
    "    X_test (pd.DataFrame) : input DataFrame with features for testing\n",
    "    y_test (pd.DataFrame) : input DataFrame with target values for testing\n",
    "\n",
    "    Output:\n",
    "    model : model trained on all available data\n",
    "    '''\n",
    "    # fit model on entire X_train_val, y_train_val datasets\n",
    "    X = pd.concat([X_train_val, X_test])\n",
    "    y = pd.concat([y_train_val, y_test])\n",
    "    model.fit(X, y.values.ravel())\n",
    "    return model\n",
    "    \n",
    "\n",
    "def save_predictions(X_pred:pd.DataFrame, model:MODEL_TYPE, filename:str) -> None:\n",
    "    '''\n",
    "    Save predictions (year 2015) to csv format, based on a provided pre-trained model, and features for predictions X_pred\n",
    "\n",
    "    Input:\n",
    "    X_pred (pd.DataFrame) : input DataFrame with features for predictions\n",
    "    model (MODEL_TYPE) : input pre-trained model, which has a predict method\n",
    "    filename (str) : Name of the file for submission\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    # predict target values\n",
    "    y_pred = model.predict(X_pred)\n",
    "    # create a DataFrame with results\n",
    "    submission = pd.DataFrame(\n",
    "        data=y_pred,\n",
    "        index=X_pred.index,\n",
    "        columns=[\"CAUSE_CODE\"]\n",
    "    ).reset_index()\n",
    "    # save to csv\n",
    "    submission.to_csv(f\"./data/4_predictions/{filename}.csv\", index=False)\n",
    "\n",
    "def save_model(model: MODEL_TYPE, file_name:str) -> None:\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    dump(model, f'./models/{file_name}.joblib') \n",
    "\n",
    "def load_model(file_name:str) -> MODEL_TYPE:\n",
    "    return load(f'./models/{file_name}.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to csv\n",
    "filename = \"submission_group_X\"\n",
    "model = fit_model_all_data(model, X_train_val, y_train_val, X_test, y_test)\n",
    "save_predictions(X_pred, model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if necessary, for example if you want to close this notebook and continue fine-tuning later\n",
    "save_model(model, file_name=\"model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if necessary for further experiments\n",
    "model = load_model('model_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability\n",
    "\n",
    "  \n",
    "Now that we have a working model, we can try to understand how its predictions are made. Beyond feature importance we can study [Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html). They allow us to see the importance of each feature as well as how each feature impacts the predictions towards a prediction or the other depending on the feature value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shapley_values(model:MODEL_TYPE)->None:\n",
    "    '''\n",
    "    Plot the shapley graph of the given model.\n",
    "    ##########################################################################\n",
    "    WARNING : Do not use it on random forest as it takes a huge amout of time.\n",
    "    ##########################################################################\n",
    "    Input:\n",
    "    model (MODEL_TYPE) : Model for which to explain the predictions\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    '''\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap.summary_plot(shap_values, X_test)\n",
    "    except (InvalidModelError, TypeError) as error:\n",
    "        explainer = shap.Explainer(model, X_test)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap.summary_plot(shap_values, X_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_values(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shapley_values(logistic_regression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f17db48aa853750bfee38181acc93506773951f4f6f179b65dfa4e5104417bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

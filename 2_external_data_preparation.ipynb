{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import check_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checks = {True:\"OK\", False: \"NOK\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Merge all csv files into a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to be used to plot on states on map, it only recognizes 2 letter codes.\n",
    "STATE2ABBREV = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_weather(city_name:str, info_cities:pd.DataFrame, date_min:str, date_max:str)-> pd.DataFrame:\n",
    "    '''\n",
    "    Creates a dataset of the daily max temperature, min temperature, precipitation\n",
    "    with added info of city lat, lon and name\n",
    "    \n",
    "    Input:\n",
    "    city_name (str) : Name of the city to process (case insensitive)\n",
    "    info_cities (pd.DataFrame) : DataFrame with the city information\n",
    "    date_min (str) : minimum date for filtering\n",
    "    date_max (str) : maximum date for filtering\n",
    "    \n",
    "    Output:\n",
    "    (pd.DataFrame) : Processed dataframe\n",
    "    \n",
    "    '''\n",
    "    file_name = info_cities[info_cities[\"Name\"].apply(lambda x : x.lower())==city_name.lower()].ID.iloc[0]\n",
    "    df = pd.read_csv(f\"./data/1_raw/cities/{file_name}.csv\", index_col=0, parse_dates=[\"Date\"])\n",
    "    # Add attributes\n",
    "    df[\"Lat\"] = info_cities[info_cities[\"ID\"]==file_name].iloc[0][\"Lat\"]\n",
    "    df[\"Lon\"] = info_cities[info_cities[\"ID\"]==file_name].iloc[0][\"Lon\"]\n",
    "    df[\"City_name\"] = info_cities[info_cities[\"ID\"]==file_name].iloc[0][\"Name\"]\n",
    "\n",
    "    # filter on max and min dates\n",
    "    df = df.loc[(df[\"Date\"] <= date_max)&(df[\"Date\"] >= date_min)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_weather(info_cities:pd.DataFrame, date_min:str, date_max:str)->pd.DataFrame:\n",
    "    '''\n",
    "    Concatenate the different cities dataframe into a single dataframe sorted by date and city name.\n",
    "    Filter between min and max dates.\n",
    "    \n",
    "    Input:\n",
    "    info_cities (pd.DataFrame) : DataFrame with the city information\n",
    "    date_min (str) : minimum date for filtering\n",
    "    date_max (str) : maximum date for filtering\n",
    "    \n",
    "    Output:\n",
    "    (pd.DataFrame) : Processed dataframe\n",
    "    \n",
    "    '''\n",
    "    city_names = info_cities[\"Name\"].unique()\n",
    "    weather_timeseries = [clean_raw_weather(city_name, info_cities, date_min, date_max) for city_name in city_names]\n",
    "    all_cities = pd.concat(weather_timeseries, axis=0).sort_values(by=[\"Date\",\"City_name\"])\n",
    "    return all_cities\n",
    "\n",
    "def prepare_demography(df:pd.DataFrame)->pd.DataFrame:\n",
    "    '''\n",
    "    Clean the demography dataset\n",
    "    \n",
    "    Input:\n",
    "    df (pd.DataFrame) : Demographics dataset\n",
    "    \n",
    "    Output:\n",
    "    (pd.DataFrame) : Processed dataframe\n",
    "    \n",
    "    '''\n",
    "    # Drop useless demographic data\n",
    "    df = df[[\"City\", \"State\", \"Median Age\", \"Total Population\", \"Average Household Size\"]]\n",
    "    # Since we do not care about racial demographics, we can drop the multiline\n",
    "    df = df.groupby('City').first()\n",
    "    # compute state Abbreviation\n",
    "    df[\"STATE_CODE\"] = df[\"State\"].map(STATE2ABBREV)\n",
    "    return df\n",
    "\n",
    "def get_merge_name(names:pd.Series)->pd.Series:\n",
    "    '''\n",
    "    Uniformize city names for simpler merging by lowering and removing all whitespaces.\n",
    "    \n",
    "    Input:\n",
    "    df (pd.Series) : Raw city names\n",
    "    \n",
    "    Output:\n",
    "    (pd.Series) : \"Clean\" city names\n",
    "    \n",
    "    '''\n",
    "    return names.apply(lambda x : \"\".join(x.lower().split())).values\n",
    "\n",
    "\n",
    "def feature_engineering_external_data(external_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Compute new features based on the original features from external_data.\n",
    "\n",
    "    Input:\n",
    "    external_data (pd.DataFrame): input DataFrame\n",
    "\n",
    "    Output:\n",
    "    (pd.DataFrame):  DataFrame with additional features\n",
    "    '''\n",
    "    # To Be Completed\n",
    "    \n",
    "    return external_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check duplicates: OK\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "date_min = \"2011-01-01\"\n",
    "date_max = \"2015-12-31\"\n",
    "\n",
    "# read files\n",
    "info_cities = pd.read_csv(\"./data/1_raw/cities/city_info.csv\", index_col=0)\n",
    "demography_raw = pd.read_csv(\"./data/1_raw/demographics/us-cities-demographics.csv\", delimiter=\";\")\n",
    "\n",
    "# cleaning temperature and precipitations\n",
    "all_cities = prepare_weather(info_cities, date_min, date_max)\n",
    "\n",
    "# cleaning demographics\n",
    "demography = prepare_demography(demography_raw)\n",
    "\n",
    "# merge external data into a single dataframe\n",
    "all_cities[\"MergeName\"] = get_merge_name(all_cities.City_name)\n",
    "demography[\"MergeName\"] = get_merge_name(pd.Series(demography.index))\n",
    "external_data = pd.merge(left=all_cities, right=demography, how=\"left\", on=[\"MergeName\"]).drop(columns=[\"MergeName\"])\n",
    "\n",
    "# feature engineering\n",
    "external_data = feature_engineering_external_data(external_data)\n",
    "\n",
    "# check duplicates\n",
    "c = checks.get(check_duplicates(external_data, [\"City_name\",\"Date\"]), False)\n",
    "print(f\"Check duplicates: {c}\")\n",
    "\n",
    "# save to csv format\n",
    "external_data.to_csv(\"./data/2_clean/external_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f17db48aa853750bfee38181acc93506773951f4f6f179b65dfa4e5104417bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

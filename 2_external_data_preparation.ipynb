{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"./figures/logo-hi-paris-retina.png\" alt=\"Logo\" width=\"280\" height=\"180\">\n",
    "  </a>\n",
    "\n",
    "  <h3 align=\"center\">Data Science Bootcamp</h3>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors : Yann Berthelot, Florian Bettini, Laure-Amélie Colin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about Data Cleaning, please refer to the `1_fires_preparation.ipynb` notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective of this lab\n",
    "======\n",
    "\n",
    "Merge external data (temperature, rainfall, demographics) into a single table. Clean the data to obtain a final dataset, without errors, duplicates, irrelevant values... ready to be analyzed\n",
    "\n",
    "\n",
    "Objectives of this Notebook\n",
    "======\n",
    "\n",
    "##### **Objectives:**\n",
    "- Read multiple external datasets\n",
    "    - temperatures and precipitations from the `./data/1_raw/cities/` folder\n",
    "    - demographics from the file `./data/1_raw/cities/us-cities-demographics.csv`\n",
    "- Merge them into a unique table\n",
    "- Save the cleaned DataFrame in `./data/2_clean/external_data.csv`\n",
    "\n",
    "\n",
    "##### **Data structure**:\n",
    "- Temperature and precipitation\n",
    "    - The `./data/1_raw/cities/` folder contains temperature and precipitation values for 210 US cities.\n",
    "    - it contains a file `./data/1_raw/cities/city_infos.csv` that maps the city name with a code (example: \"USW00094728\" for \"New York\")\n",
    "    - a README file `./data/1_raw/cities/README.txt` can give you additional information on these files.\n",
    "    - all other files are named with a city code (example: `./data/1_raw/cities/USW00094728.csv` for \"New York\") and contains historical temperature and precipitations between **1894** and **2021**, if available. They contains the following columns:\n",
    "        * `Date`: day (format YYYY-mm-dd)\n",
    "        * `tmax`: maximum temperature observed during the day (in fahrenheit)\n",
    "        * `tmin`: minimum temperature observed during the day (in fahrenheit)\n",
    "        * `prcp`: daily precipitation amount (inch per square meter, in/m²)\n",
    "- Demographics\n",
    "    - The `./data/1_raw/demographics/us-cities-demographics.csv` file contains demographic data (age, total population, etc.) for US cities. Data are taken in 2015. Columns are:\n",
    "        * `City`: city name\n",
    "        * `State`: state name (full name)\n",
    "        * `Median Age`: median age of the population\n",
    "        * `Male Population`: male population (integer)\n",
    "        * `Female Population`: female population (integer)\n",
    "        * `Total Population`: total population (integer)\n",
    "        * `Number of Veterans`: number of veterans (integer)\n",
    "        * `Foreign-born`: number of foreign-born (integer)\n",
    "        * `Average Household Size`: average size of an household\n",
    "        * `State Code`: state code (abbreviation)\n",
    "        * `Race`: most represented race\n",
    "        * `Count`: unknown column, should not be used\n",
    "\n",
    "##### One can find bellow some guidelines for this process:\n",
    "- For cities information (temperature, precipitation, latitude, longitude and city name)\n",
    "    - read a unique file with temperatures and precipitations, and filter the dates between 2011 and 2015 (included)\n",
    "    - when reading this unique file, add a column with the city attributes (latitude, longitude and city name)\n",
    "    - apply this process to read all files, and concatenate all DataFrames into a single one\n",
    "- For demographics:\n",
    "    - read the input file and keep one record per city\n",
    "    - map states' abbreviations from their fullname (with `STATE2ABBREV`)\n",
    "- For external data:\n",
    "    - merge the 2 previously created DataFrame from cities and demographics, to get a unique output DataFrame\n",
    "    - For each dataset, compute a new column with the city name, transformed without spaces and with lowercase. You can use the following function when computing this column: `\"\".join(city_name.lower().split())`\n",
    "    - Use this column when joining the 2 DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"./figures/UpToYou.png\" alt=\"Logo\" width=\"200\" height=\"280\">\n",
    "  </a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import check_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checks = {True:\"OK\", False: \"NOK\"}\n",
    "\n",
    "# Dict to be used to plot on states on map, it only recognizes 2 letter codes.\n",
    "STATE2ABBREV = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Merge all csv files into a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_weather(city_name:str, info_cities:pd.DataFrame, date_min:str, date_max:str)-> pd.DataFrame:\n",
    "    '''\n",
    "    Creates a dataset of the daily max temperature, min temperature, precipitation\n",
    "    with added info of city lat, lon and name\n",
    "    \n",
    "    Input:\n",
    "    city_name (str) : Name of the city to process (case insensitive)\n",
    "    info_cities (pd.DataFrame) : DataFrame with the city information\n",
    "    date_min (str) : minimum date for filtering\n",
    "    date_max (str) : maximum date for filtering\n",
    "    \n",
    "    Output:\n",
    "    (pd.DataFrame) : Processed dataframe\n",
    "    \n",
    "    '''\n",
    "    file_name = info_cities[info_cities[\"Name\"].apply(lambda x : x.lower())==city_name.lower()].ID.iloc[0]\n",
    "    df = pd.read_csv(f\"./data/1_raw/cities/{file_name}.csv\", index_col=0, parse_dates=[\"Date\"])\n",
    "    # Add attributes\n",
    "    df[\"Lat\"] = info_cities[info_cities[\"ID\"]==file_name].iloc[0][\"Lat\"]\n",
    "    df[\"Lon\"] = info_cities[info_cities[\"ID\"]==file_name].iloc[0][\"Lon\"]\n",
    "    df[\"City_name\"] = info_cities[info_cities[\"ID\"]==file_name].iloc[0][\"Name\"]\n",
    "\n",
    "    # filter on max and min dates\n",
    "    df = df.loc[(df[\"Date\"] <= date_max)&(df[\"Date\"] >= date_min)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_weather(info_cities:pd.DataFrame, date_min:str, date_max:str)->pd.DataFrame:\n",
    "    '''\n",
    "    Concatenate the different cities dataframe into a single dataframe sorted by date and city name.\n",
    "    Filter between min and max dates.\n",
    "    \n",
    "    Input:\n",
    "    info_cities (pd.DataFrame) : DataFrame with the city information\n",
    "    date_min (str) : minimum date for filtering\n",
    "    date_max (str) : maximum date for filtering\n",
    "    \n",
    "    Output:\n",
    "    (pd.DataFrame) : Processed dataframe\n",
    "    \n",
    "    '''\n",
    "    city_names = info_cities[\"Name\"].unique()\n",
    "    weather_timeseries = [clean_raw_weather(city_name, info_cities, date_min, date_max) for city_name in city_names]\n",
    "    all_cities = pd.concat(weather_timeseries, axis=0).sort_values(by=[\"Date\",\"City_name\"])\n",
    "    return all_cities\n",
    "\n",
    "def prepare_demography(df:pd.DataFrame)->pd.DataFrame:\n",
    "    '''\n",
    "    Clean the demography dataset\n",
    "    \n",
    "    Input:\n",
    "    df (pd.DataFrame) : Demographics dataset\n",
    "    \n",
    "    Output:\n",
    "    (pd.DataFrame) : Processed dataframe\n",
    "    \n",
    "    '''\n",
    "    # Drop useless demographic data\n",
    "    df = df[[\"City\", \"State\", \"Median Age\", \"Total Population\", \"Average Household Size\"]]\n",
    "    # Since we do not care about racial demographics, we can drop the multiline\n",
    "    df = df.groupby('City').first()\n",
    "    # compute state Abbreviation\n",
    "    df[\"STATE_CODE\"] = df[\"State\"].map(STATE2ABBREV)\n",
    "    return df\n",
    "\n",
    "def get_merge_name(names:pd.Series)->pd.Series:\n",
    "    '''\n",
    "    Uniformize city names for simpler merging by lowering and removing all whitespaces.\n",
    "    \n",
    "    Input:\n",
    "    df (pd.Series) : Raw city names\n",
    "    \n",
    "    Output:\n",
    "    (pd.Series) : \"Clean\" city names\n",
    "    \n",
    "    '''\n",
    "    return names.apply(lambda x : \"\".join(x.lower().split())).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check duplicates: OK\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "date_min = \"2011-01-01\"\n",
    "date_max = \"2015-12-31\"\n",
    "\n",
    "# read files\n",
    "info_cities = pd.read_csv(\"./data/1_raw/cities/city_info.csv\", index_col=0)\n",
    "demography_raw = pd.read_csv(\"./data/1_raw/demographics/us-cities-demographics.csv\", delimiter=\";\")\n",
    "\n",
    "# cleaning temperature and precipitations\n",
    "all_cities = prepare_weather(info_cities, date_min, date_max)\n",
    "\n",
    "# cleaning demographics\n",
    "demography = prepare_demography(demography_raw)\n",
    "\n",
    "# merge external data into a single dataframe\n",
    "all_cities[\"MergeName\"] = get_merge_name(all_cities.City_name)\n",
    "demography[\"MergeName\"] = get_merge_name(pd.Series(demography.index))\n",
    "external_data = pd.merge(left=all_cities, right=demography, how=\"left\", on=[\"MergeName\"]).drop(columns=[\"MergeName\"])\n",
    "\n",
    "# check duplicates\n",
    "c = checks.get(check_duplicates(external_data, [\"City_name\",\"Date\"]), False)\n",
    "print(f\"Check duplicates: {c}\")\n",
    "\n",
    "# save to csv format\n",
    "external_data.to_csv(\"./data/2_clean/external_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Away\n",
    "- Finding external data in a AI project can improve the scoring of the model\n",
    "- It is often necessary to transform this external data before being able to merge it\n",
    "\n",
    "### Pitfalls to avoid\n",
    "- not checking for any duplicate values after a join/merge operation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f17db48aa853750bfee38181acc93506773951f4f6f179b65dfa4e5104417bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
